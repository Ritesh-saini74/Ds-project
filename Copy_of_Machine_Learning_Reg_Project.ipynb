{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "r2jJGEOYphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritesh-saini74/Ds-project/blob/main/Copy_of_Machine_Learning_Reg_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Bike sharing demand prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Problem Statement"
      ],
      "metadata": {
        "id": "L4OrLXYXBNSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem statement for bike-sharing demand is predicting the number of bikes that will be rented from a bike-sharing system at a given time based on factors such as weather, day of the week, and time of day. The purpose is to build a predictive model that can accurately forecast bike rental demand to optimize bike allocation and improve the bike-sharing system’s overall efficiency.\n",
        "\n",
        "The problem statement may involve answering specific questions such as:\n",
        "\n",
        "What is the expected demand for bikes during peak hours, weekdays, or weekends?\n",
        "\n",
        "How does weather (e.g., wind, temperature, precipitation) affect bike rental demand?\n",
        "\n",
        "Are any specific locations or routes with higher or lower demand for bikes?\n",
        "How can we optimize the bike-sharing system to meet fluctuating demand and minimize operational costs?\n",
        "\n",
        "Can the bike-sharing system expand or improve to better serve users’ needs and promote sustainable transportation?\n",
        "\n",
        "The problem statement for bike-sharing demand analysis typically involves predicting bike rental demand and optimizing bike allocation to improve the bike-sharing system’s efficiency and sustainability.\n",
        "\n",
        "So are Aims is :\n",
        "\n",
        "To create a model of the demand for shared bikes with the available independent variables.\n",
        "\n",
        "To understand the demand dynamics of the market using the model."
      ],
      "metadata": {
        "id": "4j0lKNooCKnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Ritesh-saini74/Data-science---Project-"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "import datetime as dt\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DvSuCflPzqE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/SeoulBikeData (1).csv',encoding = 'latin1')"
      ],
      "metadata": {
        "id": "H1ISZLugvXoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset with rows and columns\n",
        "dataset.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 8760 rows and 14 columns in our data"
      ],
      "metadata": {
        "id": "kqOmwLOg1XY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset info\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset duplicate value counts\n",
        "\n",
        "x = dataset.duplicated().value_counts()\n",
        "print(f'data is duplicate {x}')\n",
        "print(f'the number of duplicate value in the data is',len(dataset[dataset.duplicated()]))"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "dataset.isna().sum()\n",
        "dataset.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "sns.heatmap(dataset.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "YApbYvL8EYaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Map represent that there is no null value in data **\n"
      ],
      "metadata": {
        "id": "i6MGrvs0-KDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Dataset contains 8760 rows and 14 columns.\n",
        "\n",
        "Data set have all the unique value.\n",
        "\n",
        "There is no null value or missing value in our data.\n",
        "\n",
        "In a day we have 24 hours and we have 365 days a year so 365 multiplied by 24 = 8760, which represents the number of line in the dataset.\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "dataset.columns"
      ],
      "metadata": {
        "id": "0ULIkRDVEtkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "\n",
        "dataset.describe(include = 'all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Date : The date of the day, during 365 days from 01/12/2017 to 30/11/2018, formating in DD/MM/YYYY, type : str, we need to convert into datetime format.\n",
        "\n",
        "Rented Bike Count : Number of rented bikes per hour which our dependent variable and we need to predict that, type : int\n",
        "\n",
        "Hour: The hour of the day, starting from 0-23 it's in a digital time format, type : int, we need to convert it into category data type.\n",
        "\n",
        "Temperature(°C): Temperature in Celsius, type : Float\n",
        "\n",
        "Humidity(%): Humidity in the air in %, type : int\n",
        "\n",
        "Wind speed (m/s) : Speed of the wind in m/s, type : Float\n",
        "\n",
        "Visibility (10m): Visibility in m, type : int\n",
        "\n",
        "Dew point temperature(°C): Temperature at the beggining of the day, type : Float\n",
        "\n",
        "Solar Radiation (MJ/m2): Sun contribution, type : Float\n",
        "\n",
        "Rainfall(mm): Amount of raining in mm, type : Float\n",
        "\n",
        "Snowfall (cm): Amount of snowing in cm, type : Float\n",
        "\n",
        "Seasons: *Season of the year, type : str, there are only 4 season's in data *.\n",
        "\n",
        "Holiday: If the day is holiday period or not, type: str\n",
        "\n",
        "Functioning Day: If the day is a Functioning Day or not, type : str"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "dataset.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rename the complex columns name\n"
      ],
      "metadata": {
        "id": "12t9t356eRBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename the complex columns name\n",
        "\n",
        "dataset=dataset.rename(columns={'Rented Bike Count':'Rented_Bike_Count',\n",
        "                                'Temperature(°C)':'Temperature',\n",
        "                                'Humidity(%)':'Humidity',\n",
        "                                'Wind speed (m/s)':'Wind_speed',\n",
        "                                'Visibility (10m)':'Visibility',\n",
        "                                'Dew point temperature(°C)':'Dew_point_temperature',\n",
        "                                'Solar Radiation (MJ/m2)':'Solar_Radiation',\n",
        "                                'Rainfall(mm)':'Rainfall',\n",
        "                                'Snowfall (cm)':'Snowfall',\n",
        "                                'Functioning Day':'Functioning_Day'})"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n"
      ],
      "metadata": {
        "id": "HjKNxglEeObl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the \"Date\" column into three \"year\",\"month\",\"day\" column\n",
        "\n",
        "dataset['Date'] = dataset['Date'].apply(lambda x: dt.datetime.strptime(x,\"%d/%m/%Y\"))"
      ],
      "metadata": {
        "id": "iJrvjtSZCm8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['year'] = dataset['Date'].dt.year\n",
        "dataset['month'] = dataset['Date'].dt.month\n",
        "dataset['day'] = dataset['Date'].dt.day_name()"
      ],
      "metadata": {
        "id": "MX99UVEUeGre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##creating a new column of \"weekdays_weekend\" and drop the column \"Date\",\"day\",\"year\"\n",
        "\n",
        "dataset['weekdays_weekend']=dataset['day'].apply(lambda x : 1 if x=='Saturday' or x=='Sunday' else 0 )\n",
        "dataset=dataset.drop(columns=['Date','day','year'],axis=1)"
      ],
      "metadata": {
        "id": "IX04kQbzedNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we convert the \"date\" column into 3 different column i.e \"year\",\"month\",\"day\".\n",
        "\n",
        "The \"year\" column in our data set is basically contain the 2 unique number contains the details of from 2017 december to 2018 november so if i consider this is a one year then we don't need the \"year\" column so we drop it.\n",
        "\n",
        "The other column \"day\", it contains the details about the each day of the month, for our relevence we don't need each day of each month data but we need the data about, if a day is a weekday or a weekend so we convert it into this format and drop the \"day\" column."
      ],
      "metadata": {
        "id": "L1nXknz-exf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "nzmYBsf4en3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['weekdays_weekend'].value_counts()"
      ],
      "metadata": {
        "id": "b1WJVPdRfLbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As \"Hour\",\"month\",\"weekdays_weekend\" column are show as a integer data type but actually it is a category data tyepe. so we need to change this data type if we not then, while doing the further anlysis and correleted with this then the values are not actually true so we can mislead by this"
      ],
      "metadata": {
        "id": "IberGJm9fTuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Change the int64 column into catagory column\n",
        "\n",
        "cols=['Hour','month','weekdays_weekend']\n",
        "for col in cols:\n",
        "  dataset[col]=dataset[col].astype('category')"
      ],
      "metadata": {
        "id": "oeza7vs6fVLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['weekdays_weekend'].unique()"
      ],
      "metadata": {
        "id": "9S3Oy7hqfjUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Firstly, we changed our data variable or features name\n",
        "*   As we see before data column is in str format or object type so we convert the date data into three columns 'Hour','month' and 'weekdays_weekend'\n",
        "* As \"Hour\",\"month\",\"weekdays_weekend\" column are show as a integer data type but actually but we want this data as a categorical data so we convert that\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our dependent variable is \"Rented Bike Count\" so we need to analysis this column with the other columns by using some visualisation plot. First we analyze the category data type then we proceed with the numerical data type"
      ],
      "metadata": {
        "id": "migjLEWkiRdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation (Month)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(12,8))\n",
        "sns.barplot(data=dataset,x='month',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Month ')"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BAR PLOT - It shows the relationship between a numeric and a categoric variable."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above bar plot we can clearly say that from the month 5 to 10 the demand of the rented bike is high as compare to other months. These months are comes inside the summer season."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation (weekdays_weekend)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(12,8))\n",
        "sns.pointplot(data=dataset,x='Hour',y='Rented_Bike_Count',hue='weekdays_weekend',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to weekdays_weekend ')"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Point plots can be more useful than bar plots for focusing comparisons between different levels of one or more categorical variables."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above point plot we can say that in the week days which represent in blue colur show that the demand of the bike higher because of the office.\n",
        "\n",
        "Peak Time are 7 am to 9 am and 5 pm to 7 pm\n",
        "\n",
        "The orange colur represent the weekend days, and it show that the demand of rented bikes are very low specially in the morning hour but when the evening start from 4 pm to 8 pm the demand slightly increases."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation (Hour)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.barplot(data=dataset,x='Hour',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Hour ')"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It allows you to compare different sets of data among different groups easily.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above plot which shows the use of rented bike according the hours and the data are from all over the year.\n",
        "\n",
        "generally people use rented bikes during their working hour from 7am to 9am and 5pm to 7pm"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation (Functioning day)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=dataset,x='Functioning_Day',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Functioning Day ')"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It allows you to compare different sets of data among different groups easily.\n"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above bar plot which shows the use of rented bike in functioning daya or not, and it clearly shows that,\n",
        "Peoples dont use reneted bikes in no functioning day."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation (Season)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(20,8))\n",
        "sns.pointplot(data=dataset,x='Hour',y='Rented_Bike_Count',hue='Seasons',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to seasons ')"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation\n",
        "fig,ax=plt.subplots(figsize=(15,8))\n",
        "sns.barplot(data=dataset,x='Seasons',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Seasons ')"
      ],
      "metadata": {
        "id": "xbx5yH0emSBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar graph and point plot chart summarises the large set of data in simple visual form.\n",
        "Bar graph displays each category of data in the frequency distribution whereas point plot gives us to better understanding the data on each and every point"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above bar plot and point plot which shows the use of rented bike in in four different seasons, and it clearly shows that,\n",
        "In summer season the use of rented bike is high and peak time is 7am-9am and 7pm-5pm.\n",
        "In winter season the use of rented bike is very low because of snowfall."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#anlysis of data by vizualisation (Holiday)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(10,8))\n",
        "sns.barplot(data=dataset,x='Holiday',y='Rented_Bike_Count',ax=ax,capsize=.2)\n",
        "ax.set(title='Count of Rented bikes acording to Holiday ')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(figsize=(12,8))\n",
        "sns.pointplot(data=dataset,x='Hour',y='Rented_Bike_Count',hue='Holiday',ax=ax)\n",
        "ax.set(title='Count of Rented bikes acording to Holiday ')"
      ],
      "metadata": {
        "id": "w3dk4KnwnXI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar graph and point plot chart summarises the large set of data in simple visual form.\n",
        "Bar graph displays each category of data in the frequency distribution whereas point plot gives us to better understanding the data on each and every point"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above bar plot and point plot which shows the use of rented bike in a holiday, and it clearly shows that,\n",
        "plot shows that in holiday people uses the rented bike from 2pm-8pm"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze of Numerical variables"
      ],
      "metadata": {
        "id": "EctO7WXinqli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Numerical Data\n",
        "\n",
        "Numerical data is a data type expressed in numbers, rather than natural language description. Sometimes called quantitative data, numerical data is always collected in number form. Numerical data differentiates itself from other number form data types with its ability to carry out arithmetic operations with these numbers."
      ],
      "metadata": {
        "id": "ucazOkJCnz9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##assign the numerical coulmn to variavle\n",
        "\n",
        "numerical_columns=list(dataset.select_dtypes(['int64','float64']).columns)\n",
        "numerical_features=pd.Index(numerical_columns)\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "jzuH5qRqn5W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing displots to analyze the distribution of all numerical features\n",
        "for col in numerical_features:\n",
        "  plt.figure(figsize=(7,5))\n",
        "  sns.distplot(x=dataset[col])\n",
        "  plt.xlabel(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distplot represents the univariate distribution of data i.e. data distribution of a variable against the density distribution"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its represent that the distribution is normal distribution or the data distribution is skewed in nature"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Analysis the Numerical vs.Rented_Bike_Count"
      ],
      "metadata": {
        "id": "zdxXBS3dozKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Temperature\"\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='Temperature',y='Rented_Bike_Count',data = dataset)\n",
        "plt.title('Temperature vs Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Form a groupby object by grouping multiple values, its a by default graph or chart\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot we see that people like to ride bikes when it is\n",
        "pretty hot around 25°C in average"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Dew_point_temperature\"\n",
        "\n",
        "dataset.groupby('Dew_point_temperature').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Form a groupby object by grouping multiple values, its a by default graph or chart\n"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot of \"Dew_point_temperature' is almost same as the 'temperature' there is some similarity present we can check it in our next step."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Solar_Radiation\"\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='Solar_Radiation',y='Rented_Bike_Count',data = dataset)\n",
        "plt.title('Solar_Radiation vs Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Form a groupby object by grouping multiple values, its a by default graph or chart\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from the above plot we see that, the amount of rented bikes is huge, when there is solar radiation, the counter of rents is around 1000"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Snowfall\"\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='Snowfall',y='Rented_Bike_Count',data = dataset)\n",
        "plt.title('Snowfall vs Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Form a groupby object by grouping multiple values, its a by default graph or chart\n"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the plot that, on the y-axis, the amount of rented bike is very low When we have more than 4 cm of snow, the bike rents is much lower"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Rainfall\"\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(x='Rainfall',y='Rented_Bike_Count',data = dataset)\n",
        "plt.title('Rainfall vs Rented Bike Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Form a groupby object by grouping multiple values, its a by default graph or chart\n"
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above plot that even if it rains a lot the demand of of rent bikes is not decreasing, here for example even if we have 20 mm of rain there is a big peak of rented bikes"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the plot to analyze the relationship between \"Rented_Bike_Count\" and \"Wind_speed\"\n",
        "\n",
        "dataset.groupby('Wind_speed').mean()['Rented_Bike_Count'].plot()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Form a groupby object by grouping multiple values, its a by default graph or chart\n"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the above plot that the demand of rented bike is uniformly distribute despite of wind speed but when the speed of wind was 7 m/s then the demand of bike also increase that clearly means peoples love to ride bikes when its little windy."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regression plot\n",
        "\n",
        "The regression plots in seaborn are primarily intended to add a visual guide that helps to emphasize patterns in a dataset during exploratory data analyses. Regression plots as the name suggests creates a regression line between 2 parameters and helps to visualize their linear relationships."
      ],
      "metadata": {
        "id": "6AX4n6E2sdTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#printing the regression plot for all the numerical features\n",
        "\n",
        "for col in numerical_features:\n",
        "  fig,ax=plt.subplots(figsize=(6,4))\n",
        "  sns.regplot(x=dataset[col],y=dataset['Rented_Bike_Count'],scatter_kws={\"color\": 'orange'}, line_kws={\"color\": \"black\"})"
      ],
      "metadata": {
        "id": "wlbcw3wCskHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is/are the insight(s) found from the chart?\n",
        "\n",
        "\n",
        "From the above regression plot of all numerical features we see that the columns 'Temperature', 'Wind_speed','Visibility', 'Dew_point_temperature', 'Solar_Radiation' are positively relation to the target variable.\n",
        "\n",
        "which means the rented bike count increases with increase of these features.\n",
        "'Rainfall','Snowfall','Humidity' these features are negatively related with the target variaable which means the rented bike count decreases when these features increase."
      ],
      "metadata": {
        "id": "sQ7G3_lSs541"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Normalise Rented_Bike_Count column data\n",
        "\n",
        "The data normalization (also referred to as data pre-processing) is a basic element of data mining. It means transforming the data, namely converting the source data in to another format that allows processing data effectively. The main purpose of data normalization is to minimize or even exclude duplicated data"
      ],
      "metadata": {
        "id": "VUX0Aa0mtHk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribution plot of Rented Bike Count\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.xlabel('Rented_Bike_Count')\n",
        "plt.ylabel('Density')\n",
        "ax=sns.distplot(dataset['Rented_Bike_Count'],hist=True ,color=\"y\")\n",
        "ax.axvline(dataset['Rented_Bike_Count'].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(dataset['Rented_Bike_Count'].median(), color='black', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fYrE_ujBtOwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is/are the insight(s) found from the chart?\n",
        "\n",
        "\n",
        "The above graph shows that Rented Bike Count has moderate right skewness. Since the assumption of linear regression is that 'the distribution of dependent variable has to be normal', so we should perform some operation to make it normal."
      ],
      "metadata": {
        "id": "yMN1eseqteP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Boxplot of Rented Bike Count to check outliers\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=dataset['Rented_Bike_Count'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tKDIBgEAtt07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is/are the insight(s) found from the chart?\n",
        "\n",
        "\n",
        "The above boxplot shows that we have detect outliers in Rented Bike Count"
      ],
      "metadata": {
        "id": "d3eSIuu-t0q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying square root to Rented Bike Count to improve skewness\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.xlabel('Rented Bike Count')\n",
        "plt.ylabel('Density')\n",
        "\n",
        "ax=sns.distplot(np.sqrt(dataset['Rented_Bike_Count']), color=\"y\")\n",
        "ax.axvline(np.sqrt(dataset['Rented_Bike_Count']).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(np.sqrt(dataset['Rented_Bike_Count']).median(), color='black', linestyle='dashed', linewidth=2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CfrQ85t-uC14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is/are the insight(s) found from the chart?\n",
        "\n",
        "\n",
        "Since we have generic rule of applying Square root for the skewed variable in order to make it normal .After applying Square root to the skewed Rented Bike Count, here we get almost normal distribution."
      ],
      "metadata": {
        "id": "hb4oIqkFuKcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#After applying sqrt on Rented Bike Count check wheater we still have outliers\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "plt.ylabel('Rented_Bike_Count')\n",
        "sns.boxplot(x=np.sqrt(dataset['Rented_Bike_Count']))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PL6LteFtuQR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.corr()"
      ],
      "metadata": {
        "id": "ab7MWz0mue09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying Square root to the Rented Bike Count column, we find that there is no outliers present."
      ],
      "metadata": {
        "id": "WZT9rQVgu6Ct"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking of Correlation between variables"
      ],
      "metadata": {
        "id": "9PMCVHmCupbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Checking in OLS Model\n",
        "\n",
        "Ordinary least squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable"
      ],
      "metadata": {
        "id": "1vdz1WLluuEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import the module\n",
        "#assign the 'x','y' value\n",
        "\n",
        "import statsmodels.api as sm\n",
        "X = dataset[[ 'Temperature','Humidity',\n",
        "       'Wind_speed', 'Visibility','Dew_point_temperature',\n",
        "       'Solar_Radiation', 'Rainfall', 'Snowfall']]\n",
        "Y = dataset['Rented_Bike_Count']\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "1JH807KVu9Y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add a constant column\n",
        "X = sm.add_constant(X)\n",
        "X"
      ],
      "metadata": {
        "id": "HlllQbcavGqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## fit a OLS model\n",
        "\n",
        "model= sm.OLS(Y, X).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8J2CufTLy1aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R sqauare and Adj Square are near to each other. 40% of variance in the Rented Bike count is explained by the model.\n",
        "\n",
        "For F statistic , P value is less than 0.05 for 5% levelof significance.\n",
        "\n",
        "P value of dew point temp and visibility are very high and they are not significant.\n",
        "\n",
        "Omnibus tests the skewness and kurtosis of the residuals. Here the value of Omnibus is high., it shows we have skewness in our data.\n",
        "\n",
        "The condition number is large, 3.11e+04. This might indicate that there are strong multicollinearity or other numerical problems\n",
        "\n",
        "Durbin-Watson tests for autocorrelation of the residuals. Here value is less than 0.5. We can say that there exists a positive auto correlation among the variables."
      ],
      "metadata": {
        "id": "UZeJk9npy8KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.corr()"
      ],
      "metadata": {
        "id": "GlgGSmKIy-uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the OLS model we find that the 'Temperature' and 'Dew_point_temperature' are highly correlated so we need to drop one of them.\n",
        "\n",
        "for droping the we check the (P>|t|) value from above table and we can see that the 'Dew_point_temperature' value is higher so we need to drop Dew_point_temperature column\n",
        "\n",
        "For clarity, we use visualisation i.e heatmap in next step"
      ],
      "metadata": {
        "id": "nGrczqm5zBeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we check correletion betweeen variables using Correlation heatmap, it is graphical representation of correlation matrix representing correlation between different variables"
      ],
      "metadata": {
        "id": "gMWAo0FhzIwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## plot the Correlation matrix\n",
        "\n",
        "plt.figure(figsize=(12,8))\n",
        "correlation=dataset.corr()\n",
        "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
        "sns.heatmap((correlation),mask=mask, annot=True,cmap='coolwarm')"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we check correletion betweeen variables using Correlation heatmap, it is graphical representation of correlation matrix representing correlation between different variables"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can observe on the heatmap that on the target variable line the most positively correlated variables to the rent are :\n",
        "\n",
        "*   the temperature\n",
        "*   the dew point temperature\n",
        "*   the solar radiation\n",
        "\n",
        "And most negatively correlated variables are:\n",
        "\n",
        "\n",
        "*   Humidity\n",
        "*   Rainfall\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above correlation heatmap, We see that there is a positive correlation between columns 'Temperature' and 'Dew point temperature' i.e 0.91 so even if we drop this column then it dont affects the outcome of our analysis. And they have the same variations.. so we can drop the column 'Dew point temperature(°C)'."
      ],
      "metadata": {
        "id": "_VG_Hlao0E6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#drop the Dew point temperature column\n",
        "\n",
        "dataset=dataset.drop(['Dew_point_temperature'],axis=1)"
      ],
      "metadata": {
        "id": "TAHU2hml0Ilh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.info()"
      ],
      "metadata": {
        "id": "j8k6knrH0Ot1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** Feature Engineering & Data Pre-processing***\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cBKbII42P78t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the dummy variables\n",
        "\n",
        "\n",
        "A dataset may contain various type of values, sometimes it consists of categorical values. So, in-order to use those categorical value for programming efficiently we create dummy variables."
      ],
      "metadata": {
        "id": "90NFUIhQQOC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign all catagoriacla features to a\n",
        "\n",
        "categorical_features=list(dataset.select_dtypes(['object','category']).columns)\n",
        "categorical_features=pd.Index(categorical_features)\n",
        "categorical_features"
      ],
      "metadata": {
        "id": "dbVuJONKQcJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One hot encoding\n",
        "\n",
        "A one hot encoding allows the representation of categorical data to be more expressive. Many machine learning algorithms cannot work with categorical data directly. The categories must be converted into numbers. This is required for both input and output variables that are categorical."
      ],
      "metadata": {
        "id": "f4oderNwQX9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a copy\n",
        "dataset_copy = dataset\n",
        "\n",
        "def one_hot_encoding(data, column):\n",
        "    data = pd.concat([data, pd.get_dummies(data[column], prefix=column, drop_first=True)], axis=1)\n",
        "    data = data.drop([column], axis=1)\n",
        "    return data\n",
        "\n",
        "for col in categorical_features:\n",
        "    dataset_copy = one_hot_encoding(dataset_copy, col)\n",
        "dataset_copy.head()"
      ],
      "metadata": {
        "id": "sCzh6NnuQCEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Test split for regression\n",
        "\n",
        "Before, fitting any model it is a rule of thumb to split the dataset into a training and test set. This means some proportions of the data will go into training the model and some portion will be used to evaluate how our model is performing on any unseen data. The proportions may vary from 60:40, 70:30, 75:25 depending on the person but mostly used is 80:20 for training and testing respectively. In this step we will split our data into training and testing set using scikit learn library."
      ],
      "metadata": {
        "id": "YzzPNs30QpCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign the value in X and Y\n",
        "\n",
        "X = dataset_copy.drop(columns=['Rented_Bike_Count'], axis=1)\n",
        "y = np.sqrt(dataset_copy['Rented_Bike_Count'])"
      ],
      "metadata": {
        "id": "OqZc-JUEQk-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "Zsc8epS8Q4c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.head()"
      ],
      "metadata": {
        "id": "dKmnBqbBQ4ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create test and train data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=0)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "nBZ1efRlQ4VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_copy.describe().columns"
      ],
      "metadata": {
        "id": "mcv2m6NvQ4RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The mean squared error (MSE) tells you how close a regression line is to a set of points. It does this by taking the distances from the points to the regression line (these distances are the “errors”) and squaring them.\n",
        "It’s called the mean squared error as you’re finding the average of a set of errors. The lower the MSE, the better the forecast.\n",
        "\n",
        "* MSE formula = (1/n) * Σ(actual – forecast)2\n",
        "Where:\n",
        "\n",
        "*   n = number of items,\n",
        "* Σ = summation notation,\n",
        "* Actual = original or observed y-value,\n",
        "* Forecast = y-value from regression.\n",
        "\n",
        "* Root Mean Square Error (RMSE) is the standard deviation of the residuals (prediction errors).\n",
        "\n",
        "* Mean Absolute Error (MAE) are metrics used to evaluate a Regression Model. ... Here, errors are the differences between the predicted values (values predicted by our regression model) and the actual values of a variable.\n",
        "\n",
        "* R-squared (R2) is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
        "\n",
        "* Formula for R-Squared\n",
        "\\begin{aligned} &\\text{R}^2 = 1 - \\frac{ \\text{Unexplained Variation} }{ \\text{Total Variation} } \\\\ \\end{aligned}\n",
        "​\n",
        "  \n",
        "* R\n",
        "2\n",
        " =1−\n",
        "Total Variation\n",
        "Unexplained Variation\n",
        "​\n",
        "\n",
        "* Adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
        "​\n"
      ],
      "metadata": {
        "id": "U3UAorvYRKbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LINEAR REGRESSION**\n"
      ],
      "metadata": {
        "id": "ZhR0pp1iRQtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression uses a linear approach to model the relationship between independent and dependent variables. In simple words its a best fit line drawn over the values of independent variables and dependent variable. In case of single variable, the formula is same as straight line equation having an intercept and slope.\n",
        "\n",
        "$$ \\text{y_pred} = \\beta_0 + \\beta_1x$$\n",
        "\n",
        "where $$\\beta_0 \\text{ and } \\beta_1$$ are intercept and slope respectively.\n",
        "\n",
        "In case of multiple features the formula translates into:\n",
        "\n",
        "$$ \\text{y_pred} = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 +\\beta_3x_3 +.....$$\n",
        "\n",
        "where x_1,x_2,x_3 are the features values and\n",
        "$$\\beta_0,\\beta_1,\\beta_2.....$$\n",
        " are weights assigned to each of the features. These become the parameters which the algorithm tries to learn using Gradient descent.\n",
        "\n",
        "Gradient descent is the process by which the algorithm tries to update the parameters using  a loss function . Loss function is nothing but the diffence between the actual values and predicted values(aka error or residuals). There are different types of loss function but this is the simplest one. Loss function summed over all observation gives the cost functions. The role of gradient descent is to update the parameters till the cost function is minimized i.e., a global minima is reached. It uses a hyperparameter 'alpha' that gives a weightage to the cost function and decides on how big the steps to take. Alpha is called as the learning rate. It is always necesarry to keep an optimal value of alpha as high and low values of alpha might make the gradient descent overshoot or get stuck at a local minima. There are also some basic assumptions that must be fulfilled before implementing this algorithm. They are:\n",
        "\n",
        "1. No multicollinearity in the dataset.\n",
        "\n",
        "2. Independent variables should show linear relationship with dv.\n",
        "\n",
        "3. Residual mean should be 0 or close to 0.\n",
        "\n",
        "4. There should be no heteroscedasticity i.e., variance should be constant along the line of best fit.\n",
        "\n",
        "\n",
        "\n",
        "Let us now implement our first model.\n",
        "We will be using LinearRegression from scikit library.\n"
      ],
      "metadata": {
        "id": "4-stS8eqRTSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *** ML Model Implementation***"
      ],
      "metadata": {
        "id": "8dsE9o2mPan3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - **Implementing Logistic Regression**\n"
      ],
      "metadata": {
        "id": "qnHpN5nbRt0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "reg= LinearRegression().fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "K8UXJD1lRoPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "\n",
        "reg.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "6IskbQmZRoL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the coefficeint\n",
        "reg.coef_"
      ],
      "metadata": {
        "id": "r4n7RJm9RoIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train=reg.predict(X_train)\n",
        "y_pred_test=reg.predict(X_test)"
      ],
      "metadata": {
        "id": "2ecICtqZSBB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error((y_train), (y_pred_train))\n",
        "print(\"MSE :\",MSE_lr)"
      ],
      "metadata": {
        "id": "O3aiyF-RRn8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)"
      ],
      "metadata": {
        "id": "titArss9SFsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_train, y_pred_train)\n",
        "print(\"MAE :\",MAE_lr)"
      ],
      "metadata": {
        "id": "JXDr1NGrSH5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score(y_train, y_pred_train)\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "UPVYQGiiSJRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like our r2 score value is 0.77 that means our model is able to capture most of the data variance. Lets save it in a dataframe for later comparisons."
      ],
      "metadata": {
        "id": "m6PkJDxcSOss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "training_df=pd.DataFrame(dict1,index=[1])"
      ],
      "metadata": {
        "id": "QiaGm9l5SL5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_lr= mean_squared_error(y_test, y_pred_test)\n",
        "print(\"MSE :\",MSE_lr)"
      ],
      "metadata": {
        "id": "3Qk-tdo4SVYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate RMSE\n",
        "RMSE_lr=np.sqrt(MSE_lr)\n",
        "print(\"RMSE :\",RMSE_lr)"
      ],
      "metadata": {
        "id": "HCET_lCGSYyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MAE\n",
        "MAE_lr= mean_absolute_error(y_test, y_pred_test)\n",
        "print(\"MAE :\",MAE_lr)"
      ],
      "metadata": {
        "id": "qoqlYcYHSa6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate r2 and adjusted r2\n",
        "r2_lr= r2_score((y_test), (y_pred_test))\n",
        "print(\"R2 :\",r2_lr)\n",
        "Adjusted_R2_lr = (1-(1-r2_score((y_test), (y_pred_test)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))\n",
        "print(\"Adjusted R2 :\",Adjusted_R2_lr )"
      ],
      "metadata": {
        "id": "VHNXOESFSent"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The r2_score for the test set is 0.78. This means our linear model is performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter)."
      ],
      "metadata": {
        "id": "_2XLNTg0SlED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Linear regression ',\n",
        "       'MAE':round((MAE_lr),3),\n",
        "       'MSE':round((MSE_lr),3),\n",
        "       'RMSE':round((RMSE_lr),3),\n",
        "       'R2_score':round((r2_lr),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_lr ),2)\n",
        "       }\n",
        "test_df=pd.DataFrame(dict2,index=[1])"
      ],
      "metadata": {
        "id": "VanDMw4ZShx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test),(y_test)-(y_pred_test))"
      ],
      "metadata": {
        "id": "yvKhXUgCSqBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(y_pred_test)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.xlabel('No of Test Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "upnEE3JvSr4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model- 2 LASSO REGRESSION"
      ],
      "metadata": {
        "id": "ML0GHJgCTdMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the Lasso model\n",
        "lasso = Lasso(alpha=1.0, max_iter=3000)\n",
        "lasso.fit(X_train, y_train)\n",
        "# Create the model score\n",
        "print(lasso.score(X_test, y_test), lasso.score(X_train, y_train))"
      ],
      "metadata": {
        "id": "UwCPy3WmSujp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_lasso=lasso.predict(X_train)\n",
        "y_pred_test_lasso=lasso.predict(X_test)"
      ],
      "metadata": {
        "id": "L3Gzx3CtTpai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_l= mean_squared_error((y_train), (y_pred_train_lasso))\n",
        "print(\"MSE :\",MSE_l)"
      ],
      "metadata": {
        "id": "FggbUVRlTrQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)"
      ],
      "metadata": {
        "id": "xVMsZzN7Ts0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_train, y_pred_train_lasso)\n",
        "print(\"MAE :\",MAE_l)"
      ],
      "metadata": {
        "id": "df4V78lWTtxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score(y_train, y_pred_train_lasso)\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l = (1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_lasso))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "LXAKh7JRTvbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like our r2 score value is 0.40 that means our model is not able to capture most of the data variance. Lets save it in a dataframe for later comparisons."
      ],
      "metadata": {
        "id": "PQoX4PyUTz79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2)\n",
        "       }\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "Pn5SvLLYTxyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_l= mean_squared_error(y_test, y_pred_test_lasso)\n",
        "print(\"MSE :\",MSE_l)"
      ],
      "metadata": {
        "id": "6ZzXWNKxT37G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate RMSE\n",
        "RMSE_l=np.sqrt(MSE_l)\n",
        "print(\"RMSE :\",RMSE_l)"
      ],
      "metadata": {
        "id": "L7bD1p_yT7Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculate MAE\n",
        "MAE_l= mean_absolute_error(y_test, y_pred_test_lasso)\n",
        "print(\"MAE :\",MAE_l)"
      ],
      "metadata": {
        "id": "rO_AQDXlT80x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate r2 and adjusted r2\n",
        "r2_l= r2_score((y_test), (y_pred_test_lasso))\n",
        "print(\"R2 :\",r2_l)\n",
        "Adjusted_R2_l=(1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_lasso)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "0A01KSxuT_cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The r2_score for the test set is 0.38. This means our linear model is not performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter)."
      ],
      "metadata": {
        "id": "x4g-twinUDce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Lasso regression ',\n",
        "       'MAE':round((MAE_l),3),\n",
        "       'MSE':round((MSE_l),3),\n",
        "       'RMSE':round((RMSE_l),3),\n",
        "       'R2_score':round((r2_l),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_l ),2),\n",
        "       }\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "tYeXliXqUBTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_pred_test_lasso))\n",
        "plt.plot(np.array((y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0fbX-r3_UKxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_lasso),(y_test-y_pred_test_lasso))"
      ],
      "metadata": {
        "id": "uEM2kG2zUNPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model - 3 RIDGE REGRESSION"
      ],
      "metadata": {
        "id": "ij_f8PSuUV5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "ridge= Ridge(alpha=0.1)\n",
        "ridge.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "RnxuJlLkUjWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "ridge.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "_qBoFw6RUjpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_ridge=ridge.predict(X_train)\n",
        "y_pred_test_ridge=ridge.predict(X_test)"
      ],
      "metadata": {
        "id": "fDFaMv2RUlGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_r= mean_squared_error((y_train), (y_pred_train_ridge))\n",
        "print(\"MSE :\",MSE_r)"
      ],
      "metadata": {
        "id": "ap526rtlUsaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)"
      ],
      "metadata": {
        "id": "nj3vyPu2Uu4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_train, y_pred_train_ridge)\n",
        "print(\"MAE :\",MAE_r)"
      ],
      "metadata": {
        "id": "braYi9bxUv-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score(y_train, y_pred_train_ridge)\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_ridge))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "0iDJOe66UxG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wf9N7kHpZp2"
      },
      "source": [
        "**Looks like our r2 score value is 0.77 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "zRJU_3YzUzNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_r= mean_squared_error(y_test, y_pred_test_ridge)\n",
        "print(\"MSE :\",MSE_r)"
      ],
      "metadata": {
        "id": "YAJFupzcU42s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate RMSE\n",
        "RMSE_r=np.sqrt(MSE_r)\n",
        "print(\"RMSE :\",RMSE_r)"
      ],
      "metadata": {
        "id": "89ypHMOAU6p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculate MAE\n",
        "MAE_r= mean_absolute_error(y_test, y_pred_test_ridge)\n",
        "print(\"MAE :\",MAE_r)\n"
      ],
      "metadata": {
        "id": "IS8kuzqDU70-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate r2 and adjusted r2\n",
        "r2_r= r2_score((y_test), (y_pred_test_ridge))\n",
        "print(\"R2 :\",r2_r)\n",
        "Adjusted_R2_r=(1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_ridge)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "bmT7JuAuU9Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9WT5KXTVDEM"
      },
      "source": [
        "**The r2_score for the test set is 0.78. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Ridge regression ',\n",
        "       'MAE':round((MAE_r),3),\n",
        "       'MSE':round((MSE_r),3),\n",
        "       'RMSE':round((RMSE_r),3),\n",
        "       'R2_score':round((r2_r),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_r ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "jXAvit3OU_Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot((y_pred_test_ridge))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IKe1XrOTVHA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_ridge),(y_test)-(y_pred_test_ridge))"
      ],
      "metadata": {
        "id": "pKnqe6DLVI5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0u5B6IDEtqd"
      },
      "source": [
        "# **ML Model - 4 ELASTIC NET REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#a * L1 + b * L2\n",
        "#alpha = a + b and l1_ratio = a / (a + b)\n",
        "elasticnet = ElasticNet(alpha=0.1, l1_ratio=0.5)"
      ],
      "metadata": {
        "id": "lGyVNsNXVKu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FIT THE MODEL\n",
        "elasticnet.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "rSicbhjYWd-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the score\n",
        "elasticnet.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "jll6RMjXWfeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the X_train and X-test value\n",
        "y_pred_train_en=elasticnet.predict(X_train)\n",
        "y_pred_test_en=elasticnet.predict(X_test)"
      ],
      "metadata": {
        "id": "0lR18UXdWhbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_e= mean_squared_error((y_train), (y_pred_train_en))\n",
        "print(\"MSE :\",MSE_e)"
      ],
      "metadata": {
        "id": "_fY7JTuUWjVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)\n"
      ],
      "metadata": {
        "id": "TmcYrH-KWkpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_train, y_pred_train_en)\n",
        "print(\"MAE :\",MAE_e)\n"
      ],
      "metadata": {
        "id": "SyfqODEnWor8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score(y_train, y_pred_train_en)\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score(y_train, y_pred_train_en))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "qacDaey0Wp0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG9fezdFpcQK"
      },
      "source": [
        "**Looks like our r2 score value is 0.62 that means our model is  able to capture most of the data variance. Lets save it in a dataframe for later comparisons.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict1={'Model':'Elastic net regression ',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "training_df=training_df.append(dict1,ignore_index=True)"
      ],
      "metadata": {
        "id": "boQpNSHoWsNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MSE\n",
        "MSE_e= mean_squared_error(y_test, y_pred_test_en)\n",
        "print(\"MSE :\",MSE_e)"
      ],
      "metadata": {
        "id": "qXYOB0F7WzHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculate RMSE\n",
        "RMSE_e=np.sqrt(MSE_e)\n",
        "print(\"RMSE :\",RMSE_e)"
      ],
      "metadata": {
        "id": "JNgKTDFVW0pT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate MAE\n",
        "MAE_e= mean_absolute_error(y_test, y_pred_test_en)\n",
        "print(\"MAE :\",MAE_e)"
      ],
      "metadata": {
        "id": "OV4Faa-7W2Nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate r2 and adjusted r2\n",
        "r2_e= r2_score((y_test), (y_pred_test_en))\n",
        "print(\"R2 :\",r2_e)\n",
        "Adjusted_R2_e=(1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )\n",
        "print(\"Adjusted R2 :\",1-(1-r2_score((y_test), (y_pred_test_en)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)) )"
      ],
      "metadata": {
        "id": "6dw2L8IwW3bT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6y00NRKptiz"
      },
      "source": [
        "**The r2_score for the test set is 0.86. This means our linear model is  performing well on the data. Let us try to visualize our residuals and see if there is heteroscedasticity(unequal variance or scatter).**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the test set metrics value in a dataframe for later comparison\n",
        "dict2={'Model':'Elastic net regression Test',\n",
        "       'MAE':round((MAE_e),3),\n",
        "       'MSE':round((MSE_e),3),\n",
        "       'RMSE':round((RMSE_e),3),\n",
        "       'R2_score':round((r2_e),3),\n",
        "       'Adjusted R2':round((Adjusted_R2_e ),2)}\n",
        "test_df=test_df.append(dict2,ignore_index=True)"
      ],
      "metadata": {
        "id": "Tszf47MJW5jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the figure\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.plot(np.array(y_pred_test_en))\n",
        "plt.plot((np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z5EzYpC3XAIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Heteroscadacity\n",
        "plt.scatter((y_pred_test_en),(y_test)-(y_pred_test_en))"
      ],
      "metadata": {
        "id": "6mYV16osXBne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Conclusion***"
      ],
      "metadata": {
        "id": "6AXSk2iLYQOA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKrUqsOrcRJ7"
      },
      "source": [
        "During the time of our analysis, we initially did EDA on all the features of our datset. We first analysed our dependent variable, 'Rented Bike Count' and also transformed it. Next we analysed categorical variable and dropped the variable who had majority of one class, we also analysed numerical variable, found out the correlation, distribution and their relationship with the dependent variable. We also removed some numerical features who had mostly 0 values and hot encoded the categorical variables.\n",
        "\n",
        "Next we implemented 4 machine learning algorithms Linear Regression,lasso,ridge,elasticnet. The results of our evaluation are:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# displaying the results of evaluation metric values for all models\n",
        "result=pd.concat([training_df,test_df],keys=['Training set','Test set'])\n",
        "result"
      ],
      "metadata": {
        "id": "HaBdvE4wXDnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVIJju-bG0vc"
      },
      "source": [
        "\n",
        "• No overfitting is seen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oXsTmjB0lJ-"
      },
      "source": [
        "\n",
        "However, this is not the ultimate end. As this data is time dependent, the values for variables like temperature, windspeed, solar radiation etc., will not always be consistent. Therefore, there will be scenarios where the model might not perform well. As Machine learning is an exponentially evolving field, we will have to be prepared for all contingencies and also keep checking our model from time to time. Therefore, having a quality knowledge and keeping pace with the ever evolving ML field would surely help one to stay a step ahead in future."
      ]
    }
  ]
}